{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.fx import symbolic_trace\n",
    "from enum import Enum\n",
    "import torch.fx as fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the convolutional neural network\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc = nn.Linear(400, 120)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.maxpool1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.maxpool2(out)\n",
    "\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1319, -0.1176, -0.1194,  0.1197,  0.0881, -0.2426, -0.1008,  0.0876,\n",
      "         -0.0484, -0.0988]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = LeNet5(num_classes=10) # 10 for MNIST digit recognition\n",
    "x = torch.randn(1, 1, 32, 32)\n",
    "y = model(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %conv1 : [num_users=1] = call_module[target=conv1](args = (%x,), kwargs = {})\n",
      "    %bn1 : [num_users=1] = call_module[target=bn1](args = (%conv1,), kwargs = {})\n",
      "    %relu1 : [num_users=1] = call_module[target=relu1](args = (%bn1,), kwargs = {})\n",
      "    %maxpool1 : [num_users=1] = call_module[target=maxpool1](args = (%relu1,), kwargs = {})\n",
      "    %conv2 : [num_users=1] = call_module[target=conv2](args = (%maxpool1,), kwargs = {})\n",
      "    %bn2 : [num_users=1] = call_module[target=bn2](args = (%conv2,), kwargs = {})\n",
      "    %relu2 : [num_users=1] = call_module[target=relu2](args = (%bn2,), kwargs = {})\n",
      "    %maxpool2 : [num_users=2] = call_module[target=maxpool2](args = (%relu2,), kwargs = {})\n",
      "    %size : [num_users=1] = call_method[target=size](args = (%maxpool2, 0), kwargs = {})\n",
      "    %reshape : [num_users=1] = call_method[target=reshape](args = (%maxpool2, %size, -1), kwargs = {})\n",
      "    %fc : [num_users=1] = call_module[target=fc](args = (%reshape,), kwargs = {})\n",
      "    %relu : [num_users=1] = call_module[target=relu](args = (%fc,), kwargs = {})\n",
      "    %fc1 : [num_users=1] = call_module[target=fc1](args = (%relu,), kwargs = {})\n",
      "    %relu1_1 : [num_users=1] = call_module[target=relu1](args = (%fc1,), kwargs = {})\n",
      "    %fc2 : [num_users=1] = call_module[target=fc2](args = (%relu1_1,), kwargs = {})\n",
      "    return fc2\n"
     ]
    }
   ],
   "source": [
    "# Symbolic tracing frontend - captures the semantics of the module\n",
    "symbolic_traced: torch.fx.GraphModule = symbolic_trace(model)\n",
    "\n",
    "# High-level intermediate representation (IR) - Graph representation\n",
    "print(symbolic_traced.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode       name      target    args                  kwargs\n",
      "-----------  --------  --------  --------------------  --------\n",
      "placeholder  x         x         ()                    {}\n",
      "call_module  conv1     conv1     (x,)                  {}\n",
      "call_module  bn1       bn1       (conv1,)              {}\n",
      "call_module  relu1     relu1     (bn1,)                {}\n",
      "call_module  maxpool1  maxpool1  (relu1,)              {}\n",
      "call_module  conv2     conv2     (maxpool1,)           {}\n",
      "call_module  bn2       bn2       (conv2,)              {}\n",
      "call_module  relu2     relu2     (bn2,)                {}\n",
      "call_module  maxpool2  maxpool2  (relu2,)              {}\n",
      "call_method  size      size      (maxpool2, 0)         {}\n",
      "call_method  reshape   reshape   (maxpool2, size, -1)  {}\n",
      "call_module  fc        fc        (reshape,)            {}\n",
      "call_module  relu      relu      (fc,)                 {}\n",
      "call_module  fc1       fc1       (relu,)               {}\n",
      "call_module  relu1_1   relu1     (fc1,)                {}\n",
      "call_module  fc2       fc2       (relu1_1,)            {}\n",
      "output       output    output    (fc2,)                {}\n"
     ]
    }
   ],
   "source": [
    "symbolic_traced.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call_module conv1 (x,) Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "call_module bn1 (conv1,) BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "<class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "call_module relu1 (bn1,) ReLU()\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "call_module maxpool1 (relu1,) MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "call_module conv2 (maxpool1,) Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "call_module bn2 (conv2,) BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "<class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "call_module relu2 (bn2,) ReLU()\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "call_module maxpool2 (relu2,) MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "call_method size (maxpool2, 0)\n",
      "call_method reshape (maxpool2, size, -1)\n",
      "call_module fc (reshape,) Linear(in_features=400, out_features=120, bias=True)\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "call_module relu (fc,) ReLU()\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "call_module fc1 (relu,) Linear(in_features=120, out_features=84, bias=True)\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "call_module relu1 (fc1,) ReLU()\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "call_module fc2 (relu1_1,) Linear(in_features=84, out_features=10, bias=True)\n",
      "<class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "for node in symbolic_traced.graph.nodes:\n",
    "    if node.op == \"call_module\":\n",
    "        print(node.op, node.target, node.args, getattr(symbolic_traced, node.target))\n",
    "        print(type(getattr(symbolic_traced, node.target)))\n",
    "    elif node.op == \"call_method\":\n",
    "        print(node.op, node.target, node.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRS Sizes: {'conv1': torch.Size([1, 6, 28, 28]), 'bn1': torch.Size([1, 6, 28, 28]), 'relu1': torch.Size([1, 84]), 'maxpool1': torch.Size([1, 6, 14, 14]), 'conv2': torch.Size([1, 16, 10, 10]), 'bn2': torch.Size([1, 16, 10, 10]), 'relu2': torch.Size([1, 16, 10, 10]), 'maxpool2': torch.Size([1, 16, 5, 5]), 'fc': torch.Size([1, 120]), 'relu': torch.Size([1, 120]), 'fc1': torch.Size([1, 84]), 'fc2': torch.Size([1, 10])}\n"
     ]
    }
   ],
   "source": [
    "# Helper function that calculates the size of each layer in the ECG\n",
    "def calculate_IRS_SIZE():\n",
    "    traced = symbolic_trace(model)\n",
    "    module_to_size = {}\n",
    "\n",
    "    for node in traced.graph.nodes:\n",
    "        if node.op == \"call_module\":\n",
    "            module = getattr(traced, node.target)\n",
    "\n",
    "            def make_hook(name):\n",
    "                def hook(module, input, output):\n",
    "                    module_to_size[name] = output.shape\n",
    "                return hook\n",
    "\n",
    "            module.register_forward_hook(make_hook(node.target))\n",
    "    \n",
    "    traced(x)\n",
    "    return module_to_size\n",
    "\n",
    "IRS_sizes = calculate_IRS_SIZE()\n",
    "print(\"IRS Sizes:\", IRS_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[conv1, bn1, relu1, maxpool1, conv2, bn2, relu2, maxpool2, fc, relu, fc1, relu1_1, fc2]\n"
     ]
    }
   ],
   "source": [
    "all_operators = [node for node in symbolic_traced.graph.nodes if node.op == \"call_module\" or node.op == \"call_function\"]\n",
    "print(all_operators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 [bn1] [x]\n",
      "bn1 [relu1] [conv1]\n",
      "relu1 [maxpool1] [bn1]\n",
      "maxpool1 [conv2] [relu1]\n",
      "conv2 [bn2] [maxpool1]\n",
      "bn2 [relu2] [conv2]\n",
      "relu2 [maxpool2] [bn2]\n",
      "maxpool2 [size, reshape] [relu2]\n",
      "fc [relu] [reshape]\n",
      "relu [fc1] [fc]\n",
      "fc1 [relu1_1] [relu]\n",
      "relu1_1 [fc2] [fc1]\n",
      "fc2 [output] [relu1_1]\n"
     ]
    }
   ],
   "source": [
    "for node in symbolic_traced.graph.nodes:\n",
    "    if node.op == \"call_module\":\n",
    "        print(node, list(node.users.keys()), node.all_input_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion Plan Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an enum of values many-to-many, one-to-one\n",
    "class mapping_type(Enum):\n",
    "    MANY_TO_MANY = 1\n",
    "    ONE_TO_ONE = 2\n",
    "    SHUFFLE = 3\n",
    "    REORGANIZE = 4\n",
    "\n",
    "mapping_type_table = {\n",
    "    torch.nn.modules.batchnorm.BatchNorm2d: mapping_type.ONE_TO_ONE,\n",
    "    torch.nn.modules.activation.ReLU: mapping_type.ONE_TO_ONE,\n",
    "    torch.nn.modules.pooling.MaxPool2d: mapping_type.MANY_TO_MANY,\n",
    "    torch.nn.modules.conv.Conv2d: mapping_type.MANY_TO_MANY,\n",
    "    torch.nn.modules.linear.Linear: mapping_type.SHUFFLE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing table lookup\n",
    "# for node in symbolic_traced.graph.nodes:\n",
    "#     if node.op == \"call_module\":\n",
    "#         print(node.op, node.target, node.args, getattr(symbolic_traced, node.target))\n",
    "#         print(mapping_type_table.get(type(getattr(symbolic_traced, node.target))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAPPING_RELATIONSHIP(Enum):\n",
    "    FUSE_BREAK = 0\n",
    "    FUSE_ONE_TO_ONE = 1\n",
    "    FUSE_MANY_TO_MANY = 2\n",
    "    FUSE_SHUFFLE = 3\n",
    "\n",
    "def mapping_check(op, successor):\n",
    "    print(\"Mapping check\")\n",
    "    print(\"op:\", op)\n",
    "    print(\"successor:\", successor)\n",
    "    print(\"op type:\", type(getattr(symbolic_traced, op.target)))\n",
    "    print(\"successor type:\", type(getattr(symbolic_traced, successor.target)))\n",
    "    op1_mapping = mapping_type_table.get(type(getattr(symbolic_traced, op.target)))\n",
    "    successor_mapping = mapping_type_table.get(type(getattr(symbolic_traced, successor.target)))\n",
    "\n",
    "    # CASES depending on op and successor\n",
    "    if op1_mapping == mapping_type.ONE_TO_ONE and successor_mapping == mapping_type.ONE_TO_ONE:\n",
    "        return MAPPING_RELATIONSHIP.FUSE_ONE_TO_ONE\n",
    "    if op1_mapping == mapping_type.MANY_TO_MANY and successor_mapping == mapping_type.MANY_TO_MANY:\n",
    "        return MAPPING_RELATIONSHIP.FUSE_BREAK\n",
    "    if op1_mapping == mapping_type.MANY_TO_MANY and successor_mapping == mapping_type.ONE_TO_ONE:\n",
    "        return MAPPING_RELATIONSHIP.FUSE_MANY_TO_MANY\n",
    "    if op1_mapping == mapping_type.ONE_TO_ONE and successor_mapping == mapping_type.MANY_TO_MANY:\n",
    "        return MAPPING_RELATIONSHIP.FUSE_MANY_TO_MANY\n",
    "    if op1_mapping == mapping_type.SHUFFLE and successor_mapping == mapping_type.SHUFFLE:\n",
    "        return MAPPING_RELATIONSHIP.FUSE_SHUFFLE\n",
    "    if op1_mapping == mapping_type.SHUFFLE and successor_mapping == mapping_type.ONE_TO_ONE:\n",
    "        return MAPPING_RELATIONSHIP.FUSE_SHUFFLE\n",
    "    if op1_mapping == mapping_type.SHUFFLE and successor_mapping == mapping_type.MANY_TO_MANY:\n",
    "        return MAPPING_RELATIONSHIP.FUSE_BREAK # TODO: this should be a fuse check\n",
    "    if op1_mapping == mapping_type.ONE_TO_ONE and successor_mapping == mapping_type.SHUFFLE:\n",
    "        return MAPPING_RELATIONSHIP.FUSE_SHUFFLE\n",
    "    if op1_mapping == mapping_type.MANY_TO_MANY and successor_mapping == mapping_type.SHUFFLE:\n",
    "        return MAPPING_RELATIONSHIP.FUSE_BREAK # TODO: this should be a fuse check\n",
    "    \n",
    "    return MAPPING_RELATIONSHIP.FUSE_BREAK # DEFAULT CASE\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfused ops:  {fc1, fc, bn2, relu1, conv1, relu2, maxpool1, fc2, relu, maxpool2, relu1_1, conv2, bn1}\n",
      "Successor:  maxpool1\n",
      "sp:  relu1\n",
      "Mapping check\n",
      "op: relu1\n",
      "successor: maxpool1\n",
      "op type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "successor type: <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "Mapping check\n",
      "op: maxpool1\n",
      "successor: conv2\n",
      "op type: <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "successor type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "Mapping check\n",
      "op: relu1\n",
      "successor: bn1\n",
      "op type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "successor type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "Mapping check\n",
      "op: relu1\n",
      "successor: conv1\n",
      "op type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "successor type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "Successor:  fc2\n",
      "sp:  relu1_1\n",
      "Mapping check\n",
      "op: relu1_1\n",
      "successor: fc2\n",
      "op type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "successor type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Mapping check\n",
      "op: relu1_1\n",
      "successor: fc1\n",
      "op type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "successor type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Mapping check\n",
      "op: relu1_1\n",
      "successor: relu\n",
      "op type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "successor type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "Mapping check\n",
      "op: relu1_1\n",
      "successor: fc\n",
      "op type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "successor type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Successor:  relu2\n",
      "sp:  bn2\n",
      "Mapping check\n",
      "op: bn2\n",
      "successor: relu2\n",
      "op type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "successor type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "Mapping check\n",
      "op: relu2\n",
      "successor: maxpool2\n",
      "op type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "successor type: <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "Mapping check\n",
      "op: bn2\n",
      "successor: conv2\n",
      "op type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "successor type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "Mapping check\n",
      "op: bn2\n",
      "successor: maxpool1\n",
      "op type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "successor type: <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "Mapping check\n",
      "op: bn2\n",
      "successor: relu1\n",
      "op type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "successor type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "Mapping check\n",
      "op: bn2\n",
      "successor: bn1\n",
      "op type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "successor type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "Mapping check\n",
      "op: bn2\n",
      "successor: conv1\n",
      "op type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "successor type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "Fused ops:  {bn2, relu1, conv1, relu2, maxpool1, maxpool2, conv2, bn1}\n",
      "Unfused ops:  set()\n"
     ]
    }
   ],
   "source": [
    "def generate_seed(nodes):\n",
    "    # Find all the one-to-one mapping operators\n",
    "    one_to_one_nodes = [node for node in nodes if mapping_type_table.get(type(getattr(symbolic_traced, node.target))) == mapping_type.ONE_TO_ONE]\n",
    "    # Using the IRS_size, return the one-to-one mapping operator with the smallest output size (take product of dimensions)\n",
    "    min_size = float('inf')\n",
    "    seed_node = None\n",
    "    for node in one_to_one_nodes:\n",
    "        size = np.prod(IRS_sizes[node.target])\n",
    "        if size < min_size:\n",
    "            min_size = size\n",
    "            seed_node = node\n",
    "    return seed_node\n",
    "    \n",
    "\n",
    "def successors(op):\n",
    "    # # Get the successors that folow the current node\n",
    "    # successors = []\n",
    "    # # Find index of the current node\n",
    "    # index = all_operators.index(op)\n",
    "    # # Successors are the nodes that follow the current node\n",
    "    # successors = all_operators[index + 1:]\n",
    "\n",
    "    # return successors\n",
    "\n",
    "    successors = list(op.users.keys())\n",
    "\n",
    "    # Remove successors that meet the conditions\n",
    "    successors = [s for s in successors if not (s.op == \"output\" or s.op == \"call_method\")]\n",
    "\n",
    "    return successors\n",
    "\n",
    "def predecessors(op):\n",
    "    # Get the predecessors that folow the current node\n",
    "    # predecessors = []\n",
    "    # # Find index of the current node\n",
    "    # index = all_operators.index(op)\n",
    "    # # Predecessors are the nodes that precede the current node\n",
    "    # predecessors = all_operators[:index]\n",
    "\n",
    "    # return predecessors\n",
    "\n",
    "    # pred = [n for n in fx.graph.map_arg(test_node.args, lambda x: x if isinstance(x, fx.Node) else None) if n]\n",
    "\n",
    "    pred = list(op.all_input_nodes)\n",
    "\n",
    "    # Check if the pred is \"placeholder\" or \"size\"/\"reshape\"\n",
    "    pred = [p for p in pred if not (p.op == \"call_method\" or p.op == \"placeholder\")]\n",
    "\n",
    "    return pred\n",
    "\n",
    "def fuse_successor(op, successor, block):\n",
    "    # Check the mapping relationship\n",
    "    relation = mapping_check(op, successor)\n",
    "\n",
    "    # no relationship exists\n",
    "    if relation == MAPPING_RELATIONSHIP.FUSE_BREAK:\n",
    "        return\n",
    "    \n",
    "    # Step 2.2: check constraint requirement\n",
    "\n",
    "    # Step 2.3: if benefit of fusion unknown, get the latency and check \n",
    "    # TODO: this only applies if the fusion is potentially beneficial\n",
    "\n",
    "    # Block is the combination of op and successor\n",
    "    block.add(successor)\n",
    "    # Step 2.4: Recursively head to successor\n",
    "    for fusing_op in successors(successor):\n",
    "        fuse_successor(successor, fusing_op, block)\n",
    "\n",
    "def fuse_predecessor(sp, predecessor, block):\n",
    "    # Check relation\n",
    "    relation = mapping_check(sp, predecessor)\n",
    "\n",
    "    # no relationship exists\n",
    "    if relation == MAPPING_RELATIONSHIP.FUSE_BREAK:\n",
    "        return\n",
    "    \n",
    "    # Step 2.2: check constraint requirement\n",
    "\n",
    "    # Step 2.3: if benefit of fusion unknown, get the latency and check\n",
    "    # TODO: this only applies if the fusion is potentially beneficial\n",
    "\n",
    "    # Block is the combination of op and successor\n",
    "    block.add(predecessor)\n",
    "\n",
    "    # Step 2.4: Recursively head to predecessor\n",
    "    for fusing_op in predecessors(predecessor):\n",
    "        fuse_predecessor(sp, fusing_op, block)\n",
    "\n",
    "unfused_ops = all_operators\n",
    "# seed_node = generate_seed(unfused_ops)\n",
    "# # print(successors(unfused_ops[0]))\n",
    "# # print(predecessors(unfused_ops[0]))\n",
    "# # print(successors(unfused_ops[4]))\n",
    "# # print(predecessors(unfused_ops[4]))\n",
    "\n",
    "# test_block = {unfused_ops[-1]}\n",
    "# test_block = {seed_node}\n",
    "# fuse_successor(seed_node, successors(seed_node)[0], test_block)\n",
    "\n",
    "# print(\"Predecssors: \", predecessors(unfused_ops[3]))\n",
    "\n",
    "# print(\"Block after fusing:\", test_block)\n",
    "\n",
    "# test_node = unfused_ops[-1]\n",
    "# # Print node successors using users and keys\n",
    "# print(\"Successors: \", successors(test_node))\n",
    "\n",
    "# test_node = unfused_ops[3]\n",
    "# print(\"Successors: \", successors(test_node))\n",
    "# test_node = unfused_ops[4]\n",
    "# print(\"Predecessors: \", predecessors(test_node))\n",
    "\n",
    "# test_node = unfused_ops[7]\n",
    "# print(\"Successors: \", successors(test_node))\n",
    "\n",
    "# test_node = unfused_ops[8]\n",
    "# print(\"Predecessors: \", predecessors(test_node))\n",
    "\n",
    "# # <Algorithm Entry>\n",
    "unfused_ops = set(all_operators)\n",
    "print(\"Unfused ops: \", unfused_ops)\n",
    "# Step 1: start fuse from the selected seed\n",
    "while (sp := generate_seed(unfused_ops)):\n",
    "    block = {sp}\n",
    "    # Step 2: head to successor\n",
    "    for successor in successors(sp):\n",
    "        print(\"Successor: \", successor)\n",
    "        print(\"sp: \", sp)\n",
    "        fuse_successor(sp, successor, block)\n",
    "    # Step 3: head to predecessors\n",
    "    for predecessor in predecessors(sp):\n",
    "        fuse_predecessor(sp, predecessor, block)\n",
    "    unfused_ops -= block\n",
    "    \n",
    "print(\"Fused ops: \", block)\n",
    "print(\"Unfused ops: \", unfused_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d -> torch.Size([1, 6, 28, 28])\n",
      "BatchNorm2d -> torch.Size([1, 6, 28, 28])\n",
      "ReLU -> torch.Size([1, 84])\n",
      "MaxPool2d -> torch.Size([1, 6, 14, 14])\n",
      "Conv2d -> torch.Size([1, 16, 10, 10])\n",
      "BatchNorm2d -> torch.Size([1, 16, 10, 10])\n",
      "ReLU -> torch.Size([1, 16, 10, 10])\n",
      "MaxPool2d -> torch.Size([1, 16, 5, 5])\n",
      "Linear -> torch.Size([1, 120])\n",
      "ReLU -> torch.Size([1, 120])\n",
      "Linear -> torch.Size([1, 84])\n",
      "Linear -> torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store output sizes\n",
    "layer_outputs = {}\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    layer_outputs[module] = output.shape\n",
    "\n",
    "# Register hooks on all modules\n",
    "hooks = []\n",
    "for name, module in model.named_modules():\n",
    "    if not isinstance(module, nn.Sequential) and not isinstance(module, LeNet5):\n",
    "        hooks.append(module.register_forward_hook(hook_fn))\n",
    "\n",
    "# Create a dummy input and pass through model\n",
    "dummy_input = torch.randn(1, 1, 32, 32)\n",
    "_ = model(dummy_input)\n",
    "\n",
    "# Print the recorded output shapes\n",
    "for layer, shape in layer_outputs.items():\n",
    "    print(f\"{layer.__class__.__name__} -> {shape}\")\n",
    "\n",
    "# Remove the hooks (clean up)\n",
    "for hook in hooks:\n",
    "    hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d -> torch.Size([1, 6, 28, 28])\n",
      "BatchNorm2d -> torch.Size([1, 6, 28, 28])\n",
      "ReLU -> torch.Size([1, 84])\n",
      "MaxPool2d -> torch.Size([1, 6, 14, 14])\n",
      "Conv2d -> torch.Size([1, 16, 10, 10])\n",
      "BatchNorm2d -> torch.Size([1, 16, 10, 10])\n",
      "ReLU -> torch.Size([1, 16, 10, 10])\n",
      "MaxPool2d -> torch.Size([1, 16, 5, 5])\n",
      "Linear -> torch.Size([1, 120])\n",
      "ReLU -> torch.Size([1, 120])\n",
      "Linear -> torch.Size([1, 84])\n",
      "Linear -> torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Print the recorded output shapes\n",
    "for layer, shape in layer_outputs.items():\n",
    "    print(f\"{layer.__class__.__name__} -> {shape}\")\n",
    "\n",
    "# Remove the hooks (clean up)\n",
    "for hook in hooks:\n",
    "    hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook called for: conv1\n",
      "Hook called for: conv1\n",
      "name bn1\n",
      "Module: bn1, Output shape: torch.Size([1, 6, 28, 28])\n",
      "Hook called for: bn1\n",
      "Hook called for: bn1\n",
      "name relu1\n",
      "Module: relu1, Output shape: torch.Size([1, 6, 28, 28])\n",
      "name relu1\n",
      "Module: relu1, Output shape: torch.Size([1, 6, 28, 28])\n",
      "Hook called for: relu1\n",
      "Hook called for: relu1\n",
      "Hook called for: relu1\n",
      "Hook called for: relu1\n",
      "Hook called for: maxpool1\n",
      "Hook called for: maxpool1\n",
      "Hook called for: conv2\n",
      "Hook called for: conv2\n",
      "name bn2\n",
      "Module: bn2, Output shape: torch.Size([1, 16, 10, 10])\n",
      "Hook called for: bn2\n",
      "Hook called for: bn2\n",
      "name relu2\n",
      "Module: relu2, Output shape: torch.Size([1, 16, 10, 10])\n",
      "Hook called for: relu2\n",
      "Hook called for: relu2\n",
      "Hook called for: maxpool2\n",
      "Hook called for: maxpool2\n",
      "Hook called for: fc\n",
      "Hook called for: fc\n",
      "name relu\n",
      "Module: relu, Output shape: torch.Size([1, 120])\n",
      "Hook called for: relu\n",
      "Hook called for: relu\n",
      "Hook called for: fc1\n",
      "Hook called for: fc1\n",
      "name relu1\n",
      "Module: relu1, Output shape: torch.Size([1, 84])\n",
      "name relu1\n",
      "Module: relu1, Output shape: torch.Size([1, 84])\n",
      "Hook called for: relu1\n",
      "Hook called for: relu1\n",
      "Hook called for: relu1\n",
      "Hook called for: relu1\n",
      "Hook called for: fc2\n",
      "Hook called for: fc2\n",
      "Result: {'conv1': torch.Size([1, 6, 28, 28]), 'bn1': torch.Size([1, 6, 28, 28]), 'relu1': torch.Size([1, 84]), 'maxpool1': torch.Size([1, 6, 14, 14]), 'conv2': torch.Size([1, 16, 10, 10]), 'bn2': torch.Size([1, 16, 10, 10]), 'relu2': torch.Size([1, 16, 10, 10]), 'maxpool2': torch.Size([1, 16, 5, 5]), 'fc': torch.Size([1, 120]), 'relu': torch.Size([1, 120]), 'fc1': torch.Size([1, 84]), 'fc2': torch.Size([1, 10])}\n"
     ]
    }
   ],
   "source": [
    "traced = symbolic_trace(model)\n",
    "module_to_size = {}\n",
    "\n",
    "for node in traced.graph.nodes:\n",
    "    if node.op == \"call_module\":\n",
    "        module = getattr(traced, node.target)\n",
    "\n",
    "        def make_hook(name):\n",
    "            def hook(module, input, output):\n",
    "                module_to_size[name] = output.shape\n",
    "            return hook\n",
    "\n",
    "        module.register_forward_hook(make_hook(node.target))\n",
    "\n",
    "x = torch.randn(1, 1, 32, 32)\n",
    "traced(x)\n",
    "\n",
    "print(\"Result:\", module_to_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
